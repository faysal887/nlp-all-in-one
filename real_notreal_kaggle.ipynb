{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############## to prevent to restart kernel when any changes are made to any imported file ##############\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "############## to import any file from some other directory ##############\n",
    "# sys.path.append(\"/tmp/fastai/old\")\n",
    "\n",
    "############### to stop printing warnings ##############\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_colwidth', 10000)\n",
    "pd.set_option('display.width', 1000)\n",
    "############### to increase cells width ##############\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))\n",
    "\n",
    "############### to enable collapsible Headings and Functions ##############\n",
    "# !pip install jupyter_contrib_nbextensions\n",
    "# !jupyter contrib nbextension install --user\n",
    "# !jupyter nbextensions_configurator enable --user\n",
    "# !jupyter nbextension enable codefolding/main\n",
    "# search collapsible to enable\n",
    "\n",
    "############### enable dark theme ##############\n",
    "# !pip install jupyterthemes\n",
    "# !jt -t monokai\n",
    "# !jt -r\n",
    "# monokai\n",
    "# solarizedd\n",
    "\n",
    "############### enable dark theme ##############\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "# use progress_apply to use\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import fasttext, string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy import geocoders\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import ast\n",
    "import folium\n",
    "from folium import plugins\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from plotly import tools\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import textstat\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import plotly.figure_factory as ff\n",
    "py.init_notebook_mode(connected=True)\n",
    "from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from statistics import *\n",
    "import concurrent.futures\n",
    "import time\n",
    "# import pyLDAvis.sklearn\n",
    "from pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_readability(a,b,title,bins=0.1,colors=['#3A4750', '#F64E8B']):\n",
    "    trace1 = ff.create_distplot([a,b], [\" Real disaster tweets\",\"Not real disaster tweets\"], bin_size=bins, colors=colors, show_rug=False)\n",
    "    trace1['layout'].update(title=title)\n",
    "    py.iplot(trace1, filename='Distplot')\n",
    "    table_data= [[\"Statistical Measures\",\" Not real disaster tweets\",\"real disaster tweets\"],\n",
    "                [\"Mean\",mean(a),mean(b)],\n",
    "                [\"Standard Deviation\",pstdev(a),pstdev(b)],\n",
    "                [\"Variance\",pvariance(a),pvariance(b)],\n",
    "                [\"Median\",median(a),median(b)],\n",
    "                [\"Maximum value\",max(a),max(b)],\n",
    "                [\"Minimum value\",min(a),min(b)]]\n",
    "    trace2 = ff.create_table(table_data)\n",
    "    py.iplot(trace2, filename='Table')\n",
    "\n",
    "punctuations = string.punctuation\n",
    "stopwords = list(STOP_WORDS)\n",
    "\n",
    "parser = English()\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n",
    "    mytokens = \" \".join([i for i in mytokens])\n",
    "    return mytokens\n",
    "\n",
    "import re\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def removeurl(raw_text):\n",
    "    clean_text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', raw_text, flags=re.MULTILINE)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pickle(fname, data):\n",
    "    with open(fname+'.pickle', 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        \n",
    "def read_pickle(fname):\n",
    "    with open(fname+'.pickle', 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 5), (3263, 4))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv(f\"{PATH}train.csv\")\n",
    "test=pd.read_csv(f\"{PATH}test.csv\")\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                                                                                                                   text  target\n",
       "0   1     NaN      NaN                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all       1\n",
       "1   4     NaN      NaN                                                                                                 Forest fire near La Ronge Sask. Canada       1\n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected       1\n",
       "3   6     NaN      NaN                                                                      13,000 people receive #wildfires evacuation orders in California        1\n",
       "4   7     NaN      NaN                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school        1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=train.copy()\n",
    "df.fillna(\"missing\", inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"location\"]=df.location.str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling & Stats\n",
    "- Pandas profiling is a module to do all the initial EDA for your data. It generates the report which we can save in HTML and open in browser or print the report in jupyter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = df.profile_report(title='Pandas Profiling Report',style={'full_width':True})\n",
    "profile.to_file(output_file=f\"{PATH}train_profile.html\")\n",
    "# profile # show images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/overview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/corr.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![image.png](attachment:image.png) -->\n",
    "<div>\n",
    "<!-- <img src=\"attachment:image.png\" width=\"500\"/> -->\n",
    "<!-- <img src=\"attachment:image.png\" width=\"800\" /> -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/missing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    text  target\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all       1\n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected       1\n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California        1\n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school        1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"text\", \"target\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts().plot.bar(title='Frequency dist of Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/frq_dist_targets.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_srs = df['target'].value_counts()\n",
    "trace = go.Bar(\n",
    "    x=cnt_srs.index,\n",
    "    y=cnt_srs.values,\n",
    "    marker=dict(\n",
    "        color=cnt_srs.values,\n",
    "        colorscale = 'Jet',\n",
    "        reversescale = True\n",
    "    ),\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Target Count',\n",
    "    font=dict(size=18)\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=\"TargetCount\")\n",
    "\n",
    "## target distribution ##\n",
    "labels = (np.array(cnt_srs.index))\n",
    "sizes = (np.array((cnt_srs / cnt_srs.sum())*100))\n",
    "\n",
    "trace = go.Pie(labels=labels, values=sizes)\n",
    "layout = go.Layout(\n",
    "    title='Target distribution',\n",
    "    font=dict(size=18),\n",
    "    width=600,\n",
    "    height=600,\n",
    ")\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=\"usertype\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/target_count.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/target_dist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_ = train['location'].value_counts()\n",
    "cnt_.reset_index()\n",
    "cnt_ = cnt_[:20,]\n",
    "trace1 = go.Bar(\n",
    "                x = cnt_.index,\n",
    "                y = cnt_.values,\n",
    "                name = \"Number of tweets in dataset according to location\",\n",
    "                marker = dict(color = 'rgba(200, 74, 55, 0.5)',\n",
    "                             line=dict(color='rgb(0,0,0)',width=1.5)),\n",
    "                )\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(barmode = \"group\",title = 'Number of tweets in dataset according to location')\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/location_dist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_df = train[train[\"target\"]==1]\n",
    "train0_df = train[train[\"target\"]==0]\n",
    "cnt_1 = train1_df['location'].value_counts()\n",
    "cnt_1.reset_index()\n",
    "cnt_1 = cnt_1[:20,]\n",
    "\n",
    "cnt_0 = train0_df['location'].value_counts()\n",
    "cnt_0.reset_index()\n",
    "cnt_0 = cnt_0[:20,]\n",
    "\n",
    "trace1 = go.Bar(\n",
    "                x = cnt_1.index,\n",
    "                y = cnt_1.values,\n",
    "                name = \"Number of tweets about real disaster location wise\",\n",
    "                marker = dict(color = 'rgba(255, 74, 55, 0.5)',\n",
    "                             line=dict(color='rgb(0,0,0)',width=1.5)),\n",
    "                )\n",
    "trace0 = go.Bar(\n",
    "                x = cnt_0.index,\n",
    "                y = cnt_0.values,\n",
    "                name = \"Number of tweets other than real disaster location wise\",\n",
    "                marker = dict(color = 'rgba(79, 82, 97, 0.5)',\n",
    "                             line=dict(color='rgb(0,0,0)',width=1.5)),\n",
    "                )\n",
    "\n",
    "\n",
    "data = [trace0,trace1]\n",
    "layout = go.Layout(barmode = 'stack',title = 'Number of tweets in dataset according to location')\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/twts_per_class.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Text Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['length'] = train['text'].apply(len)\n",
    "data = [\n",
    "    go.Box(\n",
    "        y=train[train['target']==0]['length'],\n",
    "        name='Fake'\n",
    "    ),\n",
    "    go.Box(\n",
    "        y=train[train['target']==1]['length'],\n",
    "        name='Real'\n",
    "    )\n",
    "]\n",
    "layout = go.Layout(\n",
    "    title = 'Comparison of text length in Tweets'\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/lengths_comparison.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word CLoud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_words = set(STOPWORDS)\n",
    "st_words.update(['https','CO','RT','Please','via','amp','place','new','ttot','best','great','top','ht','ysecrettravel','ysecrettravel_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(height=600,repeat=False,width=1400,max_words=1000,stopwords=st_words,colormap='terrain',background_color='Cyan',mode='RGBA').generate(' '.join(df[df[\"target\"]==1]['text'].dropna().astype(str)))\n",
    "plt.figure(figsize = (16,16))\n",
    "plt.imshow(wc)\n",
    "plt.title('Tweets Wordcloud')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/w.cloud1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(height=600,repeat=False,width=1400,max_words=1000,stopwords=st_words,colormap='terrain',background_color='Cyan',mode='RGBA').generate(' '.join(df[df[\"target\"]==0]['text'].dropna().astype(str)))\n",
    "plt.figure(figsize = (16,16))\n",
    "plt.imshow(wc)\n",
    "plt.title('Tweets Wordcloud')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/w.cloud2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "missing            2533\n",
       "usa                 105\n",
       "new york             77\n",
       "united states        50\n",
       "london               50\n",
       "nigeria              35\n",
       "canada               34\n",
       "worldwide            31\n",
       "uk                   30\n",
       "india                28\n",
       "los angeles, ca      28\n",
       "mumbai               24\n",
       "california           21\n",
       "washington, dc       21\n",
       "kenya                21\n",
       "everywhere           20\n",
       "chicago, il          20\n",
       "australia            19\n",
       "new york, ny         17\n",
       "los angeles          16\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.location.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">missing</th>\n",
       "      <th>0</th>\n",
       "      <td>1458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usa</th>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new york</th>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usa</th>\n",
       "      <th>0</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>london</th>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nigeria</th>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">united states</th>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>india</th>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "location      target      \n",
       "missing       0       1458\n",
       "              1       1075\n",
       "usa           1         67\n",
       "new york      0         59\n",
       "usa           0         38\n",
       "london        0         33\n",
       "nigeria       1         28\n",
       "united states 1         27\n",
       "              0         23\n",
       "india         1         22"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g=pd.DataFrame(df.groupby([\"location\", \"target\"]).size()).sort_values([0], ascending=False)\n",
    "g.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Count Vector Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "# count_vectorizer = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Call the fit() function in order to learn a vocabulary from one or more documents.\n",
    "count_vectorizer.fit(train.text)\n",
    "\n",
    "# Call the transform() function on one or more documents as needed to encode each as a vector.\n",
    "X_train_count_vec = count_vectorizer.transform(train.text)\n",
    "X_test_count_vec = count_vectorizer.transform(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "\n",
    "truncated_svd_cv = TruncatedSVD(n_components=2)\n",
    "truncated_svd_scores = truncated_svd_cv.fit_transform(X_train_count_vec)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))   \n",
    "colors = ['yellow','red']\n",
    "\n",
    "plt.scatter(truncated_svd_scores[:,0], truncated_svd_scores[:,1] ,s = 3, alpha=.8, c=train.target, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "\n",
    "ir_patch = mpatches.Patch(color='yellow',label='Not Relevant')\n",
    "\n",
    "dis_patch = mpatches.Patch(color='red',label='Disaster')\n",
    "\n",
    "plt.legend(handles=[ir_patch, dis_patch], prop={'size': 10})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/visualize_count_vec.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x21637 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 61 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count_vec[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiza TDIDF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(min_df=3,  max_features=None, strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words = 'english')\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_vectorizer.fit(train.text)\n",
    "X_train_tfidf = tfidf_vectorizer.transform(train.text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_svd_tfidf = TruncatedSVD(n_components=2)\n",
    "truncated_svd_tfidf.fit(X_train_tfidf)\n",
    "truncated_svd_tfidf_scores = truncated_svd_tfidf.transform(X_train_tfidf)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))   \n",
    "colors = ['yellow','red']\n",
    "\n",
    "plt.scatter(truncated_svd_tfidf_scores[:,0], truncated_svd_tfidf_scores[:,1],s = 3, alpha=.8, c=train.target, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "\n",
    "ir_patch = mpatches.Patch(color='yellow',label='Irrelevant')\n",
    "\n",
    "dis_patch = mpatches.Patch(color='red',label='Disaster')\n",
    "\n",
    "plt.legend(handles=[ir_patch, dis_patch], prop={'size': 10})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/visualize_tfidf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize GloVe Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-PK,en-US;q=0.9,en;q=0.8\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-data-sets/8542/11957/compressed/glove.6B.50d.txt.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1578661446&Signature=YBF%2BWdBKHfv%2FSgVnAqtVHAT4qLRiOu%2FfYM7Th7Vha4DIfrWJIbdKgGBSN1q%2FW7RgZqQBOhp5GOGe5L%2BzsV5odoQq7UqB0Zxbw0cVR%2FdVQlgxNQKaZJXDiixPbAYne0fGiuEEcmCMmH190ETodB%2B%2BgLO0I3pZQfQpMvB%2BCVYJB0HSTiCj53Fr0yCpl8A%2FUggUe2LB7%2FQyZrxfLHnIFvVg0je2Hb%2FI6FfADqUnQJv%2FPjv%2FKS178Z384yB643NOZmCDGli7ev8u8Z4bEQaCVKT4QzNOix1RP6dpZZpRll9BLV40%2FuiyJjReu9YoNg1%2BVsTZgXKx7FN4hME5JWM3ZoEp2Q%3D%3D&response-content-disposition=attachment%3B+filename%3Dglove.6B.50d.txt.zip\" -O \"glove.6B.50d.txt.zip\" -c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 209 word vectors. 1\n"
     ]
    }
   ],
   "source": [
    "# !ls ../input/glove-global-vectors-for-word-representation\n",
    "import os\n",
    "glove_dir = f'{PATH}/' # This is the folder with the dataset\n",
    "\n",
    "glove_embedding = {} # We create a dictionary of word -> embedding\n",
    "# f = open(os.path.join(glove_dir, 'glove.6B.50d.txt')) # Open file\n",
    "f = open(f'{PATH}glove.6B.50d.txt') # Open file\n",
    "\n",
    "# In the dataset, each line represents a new word embedding\n",
    "# The line starts with the word and the embedding values follow\n",
    "count=0\n",
    "try:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "        glove_embedding[word] = embedding # Add embedding to our embedding dictionary\n",
    "except:\n",
    "    count=count+1\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(glove_embedding), count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, earthquake, May, ALLAH, Forgive, us, all]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                                                   text  target  length                                                                              tokens\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all       1      69  [Our, Deeds, are, the, Reason, of, this, earthquake, May, ALLAH, Forgive, us, all]\n",
       "1   4     NaN      NaN                                 Forest fire near La Ronge Sask. Canada       1      38                                       [Forest, fire, near, La, Ronge, Sask, Canada]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def tokenizeText(df, text_column):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    df[\"tokens\"] = df[text_column].apply(tokenizer.tokenize)\n",
    "    return df\n",
    "\n",
    "train = tokenizeText(train, \"text\")\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(glove_embedding)\n",
    "all_embs = np.stack(glove_embedding.values())\n",
    "emb_mean = all_embs.mean() # Calculate mean\n",
    "emb_std = all_embs.std() # Calculate standard deviation\n",
    "emb_mean,emb_std\n",
    "# For every dimention(eg. 1 to 50), get the average of every word's dimention in the sentence.\n",
    "def get_average_word2vec(tokens_list, pretrained_word_vector, generate_missing=False, num_dims = 50):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(num_dims)\n",
    "    if generate_missing:\n",
    "        vectorized = [pretrained_word_vector[word] if word in pretrained_word_vector else np.random.rand(num_dims) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [pretrained_word_vector[word] if word in pretrained_word_vector else np.zeros(num_dims) for word in tokens_list]\n",
    "#        print(np.array(vectorized).shape)\n",
    "    length = len(vectorized)\n",
    "#    print(length)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "#    print(np.array(summed).shape)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(pretrained_word_vector, df, token_column, generate_missing=False):\n",
    "    embeddings = df[token_column].apply(lambda x: get_average_word2vec(x, pretrained_word_vector, generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "embeddings = get_word2vec_embeddings(glove_embedding, train, \"tokens\")\n",
    "np.array(embeddings).shape\n",
    "X_train_glove, X_test_glove, y_train_glove, y_test_glove = train_test_split(embeddings, train['target'], test_size=0.2, random_state=40)\n",
    "X_train_glove = np.array(X_train_glove)\n",
    "truncated_svd_glove = TruncatedSVD(n_components=2)\n",
    "truncated_svd_glove.fit(X_train_glove)\n",
    "truncated_svd_glove_scores = truncated_svd_glove.transform(X_train_glove)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))   \n",
    "colors = ['yellow','red']\n",
    "\n",
    "plt.scatter(truncated_svd_glove_scores[:,0], truncated_svd_glove_scores[:,1],s = 3, alpha=.8, c=y_train_glove, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "\n",
    "ir_patch = mpatches.Patch(color='yellow',label='Irrelevant')\n",
    "\n",
    "dis_patch = mpatches.Patch(color='red',label='Disaster')\n",
    "\n",
    "plt.legend(handles=[ir_patch, dis_patch], prop={'size': 10})\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/visual_glove_vec.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GloVe Embeddings Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-PK,en-US;q=0.9,en;q=0.8\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-data-sets/14154/19053/compressed/crawl-300d-2M.vec.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1578667588&Signature=pgcykdbRRPVFb%2FWny%2FebEcEFeuzl5gZMlOigrbB%2FJ7Ei9F%2FrIh4xoEOl2EPzjcpk4x2QPCnQO4z%2BHhx20wDxXZs5cD5w6RFOFX14q9rGM0ZPqlCRgHCQGzZ9dkquZROHan7Y%2FpmDMZg%2BJw3i7HouyLTkoyzR8PCYg%2Bk3x5m3KoM5np3Vm9NXGQGasK45lDp%2FTiHMkOcdCs7ZIPM4%2BUILkWiAyrU5rHqHwCjzL9Y9C92VZNxkan99mmQmIQ1v8anF89Ad1nX1Fbzwjh2OEr6Jn0ReQKb0ElH4gSaWtJNT3SIWzD8MCRZmGMZ79pcJKToY7iIbXhQR9VqVGGD6DRVkQA%3D%3D&response-content-disposition=attachment%3B+filename%3Dcrawl-300d-2M.vec.zip\" -O \"crawl-300d-2M.vec.zip\" -c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Embeddings cover 52.06% of Training vocab\n",
      "GloVe Embeddings cover 82.68% of Training text\n",
      "GloVe Embeddings cover 57.21% of Test vocab\n",
      "GloVe Embeddings cover 81.85% of Test text\n",
      "GloVe Embeddings cover 52.06% of Training vocab\n",
      "GloVe Embeddings cover 82.68% of Training text\n",
      "GloVe Embeddings cover 57.21% of Test vocab\n",
      "GloVe Embeddings cover 81.85% of Test text\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(tweets):\n",
    "    vocab = {}        \n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "train_tweets = train['text'].apply(lambda s: s.split()).values\n",
    "train_vocab = build_vocab(train_tweets)\n",
    "test_tweets = test['text'].apply(lambda s: s.split()).values\n",
    "test_vocab = build_vocab(test_tweets)\n",
    "\n",
    "# GloVe-300d-840B\n",
    "# FastText-Crawl-300d-2M \n",
    "embeddings_glove = np.load(f'{PATH}glove.840B.300d.pkl', allow_pickle=True)\n",
    "# embeddings_fasttext = KeyedVectors.load_word2vec_format(f'{PATH}crawl-300d-2M.vec')\n",
    "\n",
    "# Words in intersection of vocab and embeddings are stored in covered along with their counts. Words in vocab that don't exist in embeddings are stored in oov along with their counts. n_covered and n_oov are total number of counts and they are used for calculating coverage percentages.\n",
    "\n",
    "# Both GloVe and FastText embeddings have more than 50% vocab and 80% text coverage without cleaning. GloVe and FastText coverage are very close but GloVe has slightly higher coverage.\n",
    "\n",
    "def check_coverage(vocab, embeddings, embeddings_name, dataset_name):\n",
    "    covered = {}\n",
    "    oov = {}    \n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "    \n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "            \n",
    "    vocab_coverage = len(covered) / len(vocab)\n",
    "    text_coverage = (n_covered / (n_covered + n_oov))\n",
    "    print('{} Embeddings cover {:.2%} of {} vocab'.format(embeddings_name, vocab_coverage, dataset_name))\n",
    "    print('{} Embeddings cover {:.2%} of {} text'.format(embeddings_name, text_coverage, dataset_name))\n",
    "\n",
    "#     sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "#     return sorted_oov\n",
    "    return oov\n",
    "\n",
    "train_oov_glove = check_coverage(train_vocab, embeddings_glove, 'GloVe', 'Training')\n",
    "test_oov_glove = check_coverage(test_vocab, embeddings_glove, 'GloVe', 'Test')\n",
    "# train_oov_fasttext = check_coverage(train_vocab, embeddings_fasttext, 'FastText', 'Training')\n",
    "# test_oov_fasttext = check_coverage(test_vocab, embeddings_fasttext, 'FastText', 'Test')\n",
    "\n",
    "print(\"\"\"GloVe Embeddings cover 52.06% of Training vocab\n",
    "GloVe Embeddings cover 82.68% of Training text\n",
    "GloVe Embeddings cover 57.21% of Test vocab\n",
    "GloVe Embeddings cover 81.85% of Test text\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#flightcity uk', '21.462446,-158.022017', '304', '?', '??']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_grp=pd.DataFrame(df.groupby([\"location\"]).size()).reset_index(drop=False)\n",
    "top_vals=location_grp[location_grp[0]>2].location.tolist()\n",
    "top_vals[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "geolocator= Nominatim(user_agent=\"myGeocoder\")\n",
    "def get_country(addr):\n",
    "    global count\n",
    "    print(count, end=\"\\r\")\n",
    "    count=count+1\n",
    "    try:\n",
    "        loc = geolocator.geocode(addr)\n",
    "        country=loc.address.split(\",\")[-1].strip()\n",
    "        return country\n",
    "    except Exception as e:\n",
    "#         print(e)\n",
    "        return \"missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cntrs=[get_country(x) for x in top_vals]\n",
    "# country_map=dict(zip(top_vals, cntrs))\n",
    "\n",
    "# save_as_pickle(\"country_map\", country_map)\n",
    "country_map=read_pickle(f\"{PATH}country_map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_map.pop('missing', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"country\"]=df[df.location.isin(top_vals)].location.apply(get_country)\n",
    "\n",
    "df[\"country\"]=df.location.copy()\n",
    "df=df.replace({\"country\": country_map})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">missing</th>\n",
       "      <th>0</th>\n",
       "      <td>1673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">United States of America</th>\n",
       "      <th>0</th>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">United States</th>\n",
       "      <th>1</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">United Kingdom</th>\n",
       "      <th>0</th>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Canada</th>\n",
       "      <th>1</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0\n",
       "country                  target      \n",
       "missing                  0       1673\n",
       "                         1       1252\n",
       "United States of America 0        409\n",
       "                         1        281\n",
       "United States            1        185\n",
       "                         0        132\n",
       "United Kingdom           0        123\n",
       "                         1         74\n",
       "Canada                   1         59\n",
       "                         0         52"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g=pd.DataFrame(df.groupby([\"country\", \"target\"]).size()).sort_values([0], ascending=False)\n",
    "g.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Lat,Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "geolocator= Nominatim(user_agent=\"myGeocoder\")\n",
    "def get_lat_long(addr):\n",
    "    global count\n",
    "    print(count, end=\"\\r\")\n",
    "    count=count+1\n",
    "    try:\n",
    "        loc = geolocator.geocode(addr)\n",
    "        lat, long=loc.raw[\"lat\"], loc.raw[\"lon\"]\n",
    "        return [lat, long]\n",
    "    except Exception as e:\n",
    "#         print(e)\n",
    "        return [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loct=[get_lat_long(x) for x in top_vals]\n",
    "# loct_map=dict(zip(top_vals, loct))\n",
    "\n",
    "# save_as_pickle(\"loct_map\", loct_map)\n",
    "loct_map=read_pickle(f\"{PATH}loct_map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>53.0361586</td>\n",
       "      <td>-2.9796483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usa</th>\n",
       "      <td>39.7837304</td>\n",
       "      <td>-100.4458825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new york</th>\n",
       "      <td>40.7127281</td>\n",
       "      <td>-74.0060152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>london</th>\n",
       "      <td>51.5073219</td>\n",
       "      <td>-0.1276474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nigeria</th>\n",
       "      <td>9.6000359</td>\n",
       "      <td>7.9999721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0             1\n",
       "missing   53.0361586    -2.9796483\n",
       "usa       39.7837304  -100.4458825\n",
       "new york  40.7127281   -74.0060152\n",
       "london    51.5073219    -0.1276474\n",
       "nigeria    9.6000359     7.9999721"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(loct_map).T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>['53.0361586', '-2.9796483']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>['53.0361586', '-2.9796483']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  keyword location                                                                   text  target                         point\n",
       "0   1  missing  missing  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all       1  ['53.0361586', '-2.9796483']\n",
       "1   4  missing  missing                                 Forest fire near La Ronge Sask. Canada       1  ['53.0361586', '-2.9796483']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loct_map_ = {k: str(v) for k,v in loct_map.items()}\n",
    "\n",
    "df[\"point\"]=df[df[\"location\"].isin(top_vals)][\"location\"].copy()\n",
    "df=df.replace({\"point\": loct_map_})\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>[53.0361586, -2.9796483]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[53.0361586, -2.9796483]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  keyword location                                                                   text  target                     point\n",
       "0   1  missing  missing  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all       1  [53.0361586, -2.9796483]\n",
       "1   4  missing  missing                                 Forest fire near La Ronge Sask. Canada       1  [53.0361586, -2.9796483]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"point\"].fillna(\"[0,0]\", inplace=True)\n",
    "df[\"point\"]=df.point.apply(lambda s: list(ast.literal_eval(s)))\n",
    "df[df[\"point\"]!=\"missing\"].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>point</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>[53.0361586, -2.9796483]</td>\n",
       "      <td>53.0361586</td>\n",
       "      <td>-2.9796483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[53.0361586, -2.9796483]</td>\n",
       "      <td>53.0361586</td>\n",
       "      <td>-2.9796483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  keyword location                                                                   text  target                     point         lat        long\n",
       "0   1  missing  missing  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all       1  [53.0361586, -2.9796483]  53.0361586  -2.9796483\n",
       "1   4  missing  missing                                 Forest fire near La Ronge Sask. Canada       1  [53.0361586, -2.9796483]  53.0361586  -2.9796483"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['lat','long']] = pd.DataFrame(df.point.values.tolist(), index= df.index)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>location</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>53.0361586</td>\n",
       "      <td>-2.9796483</td>\n",
       "      <td>missing</td>\n",
       "      <td>2533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>New York</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>London</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             lat        long       location  count\n",
       "3332  53.0361586  -2.9796483        missing   2533\n",
       "2641           0           0            USA    104\n",
       "1824           0           0       New York     71\n",
       "2660           0           0  United States     50\n",
       "1504           0           0         London     45"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location=df.groupby(['lat','long','location']).size().reset_index(name='count').sort_values(by='count',ascending=False)\n",
    "location=location[location['count']>5]\n",
    "location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>location</th>\n",
       "      <th>count</th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>53.0361586</td>\n",
       "      <td>-2.9796483</td>\n",
       "      <td>missing</td>\n",
       "      <td>2533</td>\n",
       "      <td>Black</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>104</td>\n",
       "      <td>red</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>New York</td>\n",
       "      <td>71</td>\n",
       "      <td>brown</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>50</td>\n",
       "      <td>brown</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>London</td>\n",
       "      <td>45</td>\n",
       "      <td>grey</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             lat        long       location  count  color  size\n",
       "3332  53.0361586  -2.9796483        missing   2533  Black  12.0\n",
       "2641           0           0            USA    104    red   4.0\n",
       "1824           0           0       New York     71  brown   1.0\n",
       "2660           0           0  United States     50  brown   1.0\n",
       "1504           0           0         London     45   grey   0.1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location['color']=location['count'].apply(lambda count:\"Black\" if count>=400 else\n",
    "                                         \"green\" if count>=300 and count<400 else\n",
    "                                         \"Orange\" if count>=200 and count<300 else\n",
    "                                         \"darkblue\" if count>=150 and count<200 else\n",
    "                                         \"red\" if count>=100 and count<150 else\n",
    "                                         \"lightblue\" if count>=75 and count<100 else\n",
    "                                         \"brown\" if count>=50 and count<75 else\n",
    "                                         \"grey\")\n",
    "location['size']=location['count'].apply(lambda count:12 if count>=400 else\n",
    "                                         10 if count>=300 and count<400 else\n",
    "                                         8 if count>=200 and count<300 else\n",
    "                                         6 if count>=150 and count<200 else\n",
    "                                         4 if count>=100 and count<150 else\n",
    "                                         2 if count>=75 and count<100 else\n",
    "                                         1 if count>=50 and count<75 else\n",
    "                                         0.1)\n",
    "location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location[\"size\"]=location[\"size\"]*50\n",
    "# location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m=folium.Map([34.2012,-118.4662],zoom_start=11)\n",
    "#location=location[0:2000]\n",
    "for lat,lon,area,color,count,size in zip(location['lat'],location['long'],location['location'],location['color'],location['count'],location['size']):\n",
    "     folium.CircleMarker([lat, lon],\n",
    "                            popup=area,\n",
    "                            radius=size,\n",
    "                            color='b',\n",
    "                            fill=True,\n",
    "                            fill_opacity=0.7,\n",
    "                            fill_color=color,\n",
    "                           ).add_to(m)\n",
    "m.save(f'{PATH}tweets.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "location1=location[0:100]\n",
    "location_data = location[['lat', 'long']].as_matrix()\n",
    "\n",
    "# plot heatmap\n",
    "m.add_children(plugins.HeatMap(location_data, radius=15))\n",
    "# m.save(f'{PATH}heatmap.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/heat_map.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA\n",
      "New York\n",
      "United States\n",
      "London\n",
      "Canada\n",
      "Nigeria\n",
      "UK\n",
      "Los Angeles, CA\n",
      "India\n",
      "Mumbai\n",
      "Washington, DC\n",
      "Kenya\n",
      "Worldwide\n",
      "Chicago, IL\n",
      "Australia\n",
      "California\n",
      "New York, NY\n",
      "California, USA\n",
      "Everywhere\n",
      "Florida\n"
     ]
    }
   ],
   "source": [
    "df = train['location'].value_counts()[:20,]\n",
    "df = pd.DataFrame(df)\n",
    "df = df.reset_index()\n",
    "df.columns = ['location', 'counts'] \n",
    "geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "dictt_latitude = {}\n",
    "dictt_longitude = {}\n",
    "for i in df['location'].values:\n",
    "    print(i)\n",
    "    location = geocode(i)\n",
    "    dictt_latitude[i] = location.latitude\n",
    "    dictt_longitude[i] = location.longitude\n",
    "df['latitude']= df['location'].map(dictt_latitude)\n",
    "df['longitude'] = df['location'].map(dictt_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "map1 = folium.Map(location=[10.0, 10.0], tiles='CartoDB dark_matter', zoom_start=2.3)\n",
    "markers = []\n",
    "for i, row in df.iterrows():\n",
    "    loss = row['counts']\n",
    "    if row['counts'] > 0:\n",
    "        count = row['counts']*0.4\n",
    "    folium.CircleMarker([float(row['latitude']), float(row['longitude'])], radius=float(count), color='#ef4f61', fill=True).add_to(map1)\n",
    "map1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/map_satelitte.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "train1_df = train[train[\"target\"]==1]\n",
    "train0_df = train[train[\"target\"]==0]\n",
    "\n",
    "## custom function for ngram generation ##\n",
    "def generate_ngrams(text, n_gram=1):\n",
    "    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "## custom function for horizontal bar chart ##\n",
    "def horizontal_bar_chart(df, color):\n",
    "    trace = go.Bar(\n",
    "        y=df[\"word\"].values[::-1],\n",
    "        x=df[\"wordcount\"].values[::-1],\n",
    "        showlegend=False,\n",
    "        orientation = 'h',\n",
    "        marker=dict(\n",
    "            color=color,\n",
    "        ),\n",
    "    )\n",
    "    return trace\n",
    "\n",
    "## Get the bar chart from sincere questions ##\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in train0_df[\"text\"]:\n",
    "    for word in generate_ngrams(sent):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace0 = horizontal_bar_chart(fd_sorted.head(50), 'red')\n",
    "\n",
    "## Get the bar chart from insincere questions ##\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in train1_df[\"text\"]:\n",
    "    for word in generate_ngrams(sent):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace1 = horizontal_bar_chart(fd_sorted.head(50), 'red')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n",
    "                          subplot_titles=[\"Frequent words if tweet is not real disaster\", \n",
    "                                          \"Frequent words if tweet is real disaster\"])\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "fig.append_trace(trace1, 1, 2)\n",
    "fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
    "py.iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/word_freq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.keyword.value_counts()[:20].iplot(kind='bar', title='Top 20 keywords in text', color='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/top_kws.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bgs(mydf):\n",
    "#     freq_dict=DefaultDict()\n",
    "    freq_dict = defaultdict(int)\n",
    "\n",
    "    for sent in mydf[\"text\"]:\n",
    "        for word in generate_ngrams(sent,2):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace0 = horizontal_bar_chart(fd_sorted.head(50), 'red')\n",
    "    return trace0\n",
    "\n",
    "\n",
    "train0_df=train[train.target==0]\n",
    "train1_df=train[train.target==1]\n",
    "trace0=get_bgs(train0_df)\n",
    "trace1=get_bgs(train1_df)\n",
    "\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n",
    "                          subplot_titles=[\"Frequent words if tweet is not real disaster\", \n",
    "                                          \"Frequent words if tweet is real disaster\"])\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "fig.append_trace(trace1, 1, 2)\n",
    "fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\n",
    "py.iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/bigrms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bgs(mydf):\n",
    "#     freq_dict=DefaultDict()\n",
    "    freq_dict = defaultdict(int)\n",
    "\n",
    "    for sent in mydf[\"text\"]:\n",
    "        for word in generate_ngrams(sent,3):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace0 = horizontal_bar_chart(fd_sorted.head(50), 'red')\n",
    "    return trace0\n",
    "\n",
    "\n",
    "train0_df=train[train.target==0]\n",
    "train1_df=train[train.target==1]\n",
    "trace0=get_bgs(train0_df)\n",
    "trace1=get_bgs(train1_df)\n",
    "\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n",
    "                          subplot_titles=[\"Frequent words if tweet is not real disaster\", \n",
    "                                          \"Frequent words if tweet is real disaster\"])\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "fig.append_trace(trace1, 1, 2)\n",
    "fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\n",
    "py.iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/trigrms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "train[\"text_corr\"]=train.text.apply(lambda x: correct_spellings(x))\n",
    "\n",
    "# text = \"corect me plese caaaar\"\n",
    "# correct_spellings(text)\n",
    "# df['text']=df['text'].apply(lambda x : correct_spellings(x)#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle(f\"{PATH}train_spellcorrected.pickle\")\n",
    "train=pd.read_pickle(f\"{PATH}train_spellcorrected.pickle\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Meta Featurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in the text\n",
    "# Number of unique words in the text\n",
    "# Number of characters in the text\n",
    "# Number of stopwords\n",
    "# Number of punctuations\n",
    "# Number of upper case words\n",
    "# Number of title case words\n",
    "# Average length of the words\n",
    "train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\n",
    "test[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "train[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of meta features vs each target class (0 or 1)\n",
    "train['num_words'].loc[train['num_words']>60] = 100 #truncation for better visuals\n",
    "train['num_punctuations'].loc[train['num_punctuations']>25] = 25 #truncation for better visuals\n",
    "train['num_chars'].loc[train['num_chars']>350] = 350 #truncation for better visuals\n",
    "\n",
    "f, axes = plt.subplots(3, 1, figsize=(10,20))\n",
    "sns.boxplot(x='target', y='num_words', data=train, ax=axes[0])\n",
    "axes[0].set_xlabel('Target', fontsize=12)\n",
    "axes[0].set_title(\"Number of words in each class\", fontsize=15)\n",
    "\n",
    "sns.boxplot(x='target', y='num_chars', data=train, ax=axes[1])\n",
    "axes[1].set_xlabel('Target', fontsize=12)\n",
    "axes[1].set_title(\"Number of characters in each class\", fontsize=15)\n",
    "\n",
    "sns.boxplot(x='target', y='num_punctuations', data=train, ax=axes[2])\n",
    "axes[2].set_xlabel('Target', fontsize=12)\n",
    "axes[2].set_title(\"Number of punctuations in each class\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/meta_features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Histogram Plots of number of words per each class (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_df = train[train[\"target\"]==1]\n",
    "train0_df = train[train[\"target\"]==0]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=train1_df['num_words'],name = 'Number of words in tweets about real disaster'))\n",
    "fig.add_trace(go.Histogram(x=train0_df['num_words'],name = 'Number of words in tweets other than real disaster'))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='stack')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_8.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Histogram Plots of number of characters per each class (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=train1_df['num_chars'],name = 'Number of chars in tweets about real disaster',marker = dict(color = 'rgba(200, 100, 0, 0.8)')))\n",
    "fig.add_trace(go.Histogram(x=train0_df['num_chars'],name = 'Number of chars in tweets about real disaster',marker = dict(color = 'rgba(25, 133, 120, 0.8)')))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='stack')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_8.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Histogram Plots of number of punctuations per each class (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=train1_df['num_punctuations'],name = 'Number of punctuations in tweets about real disaster',marker = dict(color = 'rgba(97, 175, 222, 0.8)')))\n",
    "fig.add_trace(go.Histogram(x=train0_df['num_punctuations'],name = 'Number of punctuations in tweets other than real disaster',marker = dict(color = 'rgba(200, 10, 150, 0.8)')))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='stack')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_8.3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Histogram plots of number of words in train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=train['num_words'],name = 'Number of words in training tweets',marker = dict(color = 'rgba(255, 0, 0, 0.8)')))\n",
    "fig.add_trace(go.Histogram(x=test['num_words'],name = 'Number of words in testing tweets ',marker = dict(color = 'rgba(0, 187, 187, 0.8)')))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='stack')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_8.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Histogram plots of number of chars in train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=train['num_chars'],name = 'Number of chars in training tweets',marker = dict(color = 'rgba(25, 13, 8, 0.8)')))\n",
    "fig.add_trace(go.Histogram(x=test['num_chars'],name = 'Number of chars in testing tweets ',marker = dict(color = 'rgba(8, 25, 187, 0.8)')))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='stack')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_8.5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Histogram plots of number of punctuations in train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=train['num_punctuations'],name = 'Number of punctuations in training tweets',marker = dict(color = 'rgba(222, 111, 33, 0.8)')))\n",
    "fig.add_trace(go.Histogram(x=test['num_punctuations'],name = 'Number of punctuations in testing tweets ',marker = dict(color = 'rgba(33, 111, 222, 0.8)')))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='stack')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_8.6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability is the ease with which a reader can understand a written text. In natural language processing, the readability of text depends on its content. It focuses on the words we choose, and how we put them into sentences and paragraphs for the readers to comprehend.\n",
    "9.1 The Flesch Reading Ease formula\n",
    "In the Flesch reading-ease test, higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 The Flesch Reading Ease formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score - Difficulty\n",
    "- 90-100 - Very Easy\n",
    "- 80-89 - Easy\n",
    "- 70-79 - Fairly Easy\n",
    "- 60-69 - Standard\n",
    "- 50-59 - Fairly Difficult\n",
    "- 30-49 - Difficult\n",
    "- 0-29 - Very Confusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "fre_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.flesch_reading_ease))\n",
    "fre_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.flesch_reading_ease))\n",
    "plot_readability(fre_notreal,fre_real,\"Flesch Reading Ease\",20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_9.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 The Flesch-Kincaid Grade Level\n",
    "\n",
    "These readability tests are used extensively in the field of education. The \"FleschKincaid Grade Level Formula\" instead presents a score as a U.S. grade level, making it easier for teachers, parents, librarians, and others to judge the readability level of various books and texts. It can also mean the number of years of education generally required to understand this text, relevant when the formula results in a number greater than 10. The grade level is calculated with the following formula:\n",
    "\n",
    "\n",
    "A score of 9.3 means that a ninth grader would be able to read the document.\n",
    "\n",
    "Read more: Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fkg_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.flesch_kincaid_grade))\n",
    "# fkg_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.flesch_kincaid_grade))\n",
    "# plot_readability(fkg_notreal,fkg_real,\"Flesch Kincaid Grade\",4,['#C1D37F','#491F21'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_9.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 The Fog Scale (Gunning FOG Formula)\n",
    "\n",
    "In linguistics, the Gunning fog index is a readability test for English writing. The index estimates the years of formal education a person needs to understand the text on the first reading. For instance, a fog index of 12 requires the reading level of a United States high school senior (around 18 years old).\n",
    "The formula to calculate Fog scale:\n",
    "\n",
    "Read more : Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "fog_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.gunning_fog))\n",
    "fog_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.gunning_fog))\n",
    "plot_readability(fog_notreal,fog_real,\"The Fog Scale (Gunning FOG Formula)\",4,['#E2D58B','#CDE77F'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_9.3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Automated Readability Index\n",
    "Returns the ARI (Automated Readability Index) which outputs a number that approximates the grade level needed to comprehend the text.For example if the ARI is 6.5, then the grade level to comprehend the text is 6th to 7th grade.\n",
    "Formula to calculate ARI:\n",
    "\n",
    "Read More: Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.automated_readability_index))\n",
    "ari_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.automated_readability_index))\n",
    "plot_readability(ari_notreal,ari_real,\"Automated Readability Index\",10,['#488286','#FF934F'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_9.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 The Coleman-Liau Index\n",
    "Returns the grade level of the text using the Coleman-Liau Formula. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document.\n",
    "The ColemanLiau index is calculated with the following formula:\n",
    "\n",
    "L is the average number of letters per 100 words and S is the average number of sentences per 100 words.\n",
    "Read More : Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.coleman_liau_index))\n",
    "cli_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.coleman_liau_index))\n",
    "plot_readability(cli_notreal,cli_real,\"The Coleman-Liau Index\",10,['#8491A3','#2B2D42'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_9.5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6 Linsear Write Formula\n",
    "Returns the grade level of the text using the Coleman-Liau Formula. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document.\n",
    "Read More : Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwf_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.linsear_write_formula))\n",
    "lwf_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.linsear_write_formula))\n",
    "plot_readability(lwf_notreal,lwf_real,\"Linsear Write Formula\",2,['#8D99AE','#EF233C'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_9.6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7 Dale-Chall Readability Score\n",
    "Different from other tests, since it uses a lookup table of the most commonly used 3000 English words. Thus it returns the grade level using the New Dale-Chall Formula.\n",
    "The formula for calculating the raw score of the DaleChall readability score is given below:\n",
    "\n",
    "\n",
    "Score - Understood by\n",
    "\n",
    "4.9 or lower - average 4th-grade student or lower\n",
    "5.05.9 - average 5th or 6th-grade student\n",
    "6.06.9 - average 7th or 8th-grade student\n",
    "7.07.9 - average 9th or 10th-grade student\n",
    "8.08.9 - average 11th or 12th-grade student\n",
    "9.09.9 - average 13th to 15th-grade (college) student\n",
    "\n",
    "Read More : Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcr_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.dale_chall_readability_score))\n",
    "dcr_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.dale_chall_readability_score))\n",
    "plot_readability(dcr_notreal,dcr_real,\"Dale-Chall Readability Score\",1,['#C65D17','#DDB967'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_9.7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.8 Readability Consensus based upon all the above tests\n",
    "Based upon all the above tests, returns the estimated school grade level required to understand the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus_all(text):\n",
    "    return textstat.text_standard(text,float_output=True)\n",
    "\n",
    "con_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(consensus_all))\n",
    "con_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(consensus_all))\n",
    "plot_readability(con_notreal,con_real,\"Readability Consensus based upon all the above tests\",2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/hist_9.8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis - AllenNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/sst-2-basic-classifier-glove-2019.06.27.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7613/7613 [03:46<00:00, 33.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment  target\n",
       "negative   0         1959\n",
       "           1         1328\n",
       "positive   0         2383\n",
       "           1         1943\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df[\"sentiment\"]=df.text.progress_apply(lambda x: predictor.predict(sentence=x))\n",
    "df=df.rename(columns={\"sentiment\": \"sentiment_result\"})\n",
    "df[\"sentiment\"]=df.sentiment_result.apply(lambda x: x[\"label\"])\n",
    "df.replace({\"sentiment\": {\"1\": \"positive\", \"0\": \"negative\"}}, inplace=True)\n",
    "df.groupby([\"sentiment\", \"target\"]).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7613/7613 [00:58<00:00, 137.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment_kw  target\n",
       "negative      0         1959\n",
       "              1         1328\n",
       "positive      0         2383\n",
       "              1         1943\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment_kw\"]=df.keyword.progress_apply(lambda x: predictor.predict(sentence=x))\n",
    "df=df.rename(columns={\"sentiment_kw\": \"sentiment_result_kw\"})\n",
    "df[\"sentiment_kw\"]=df.sentiment_result.apply(lambda x: x[\"label\"])\n",
    "df.replace({\"sentiment_kw\": {\"1\": \"positive\", \"0\": \"negative\"}}, inplace=True)\n",
    "df.groupby([\"sentiment_kw\", \"target\"]).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>keyword</th>\n",
       "      <th>sentiment_kw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, sentiment, keyword, sentiment_kw]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.sentiment!=df.sentiment_kw][[\"text\", \"sentiment\", \"keyword\", \"sentiment_kw\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>keyword</th>\n",
       "      <th>sentiment_kw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C</td>\n",
       "      <td>positive</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw</td>\n",
       "      <td>positive</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi</td>\n",
       "      <td>positive</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>positive</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N</td>\n",
       "      <td>positive</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>@PhDSquares #mufc they've built so much hype around new acquisitions but I doubt they will set the EPL ablaze this season.</td>\n",
       "      <td>positive</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>INEC Office in Abia Set Ablaze - http://t.co/3ImaomknnA</td>\n",
       "      <td>positive</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Barbados #Bridgetown JAMAICA  Two cars set ablaze: SANTA CRUZ  Head of the St Elizabeth Police Superintende...  http://t.co/wDUEaj8Q4J</td>\n",
       "      <td>positive</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ablaze for you Lord :D</td>\n",
       "      <td>positive</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http://t.co/3Tj8ZjiN21 http://t.co/YDUiXEfIpE http://t.co/LxTjc87KLS #nsfw</td>\n",
       "      <td>negative</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>on the outside you're ablaze and alive\\nbut you're dead inside</td>\n",
       "      <td>positive</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Had an awesome time visiting the CFC head office the ancop site and ablaze. Thanks to Tita Vida for taking care of us ??</td>\n",
       "      <td>negative</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SOOOO PUMPED FOR ABLAZE ???? @southridgelife</td>\n",
       "      <td>negative</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>I wanted to set Chicago ablaze with my preaching... But not my hotel! http://t.co/o9qknbfOFX</td>\n",
       "      <td>negative</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>I gained 3 followers in the last week. You? Know your stats and grow with http://t.co/TIyUliF5c6</td>\n",
       "      <td>negative</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>How the West was burned: Thousands of wildfires ablaze in California alone http://t.co/vl5TBR3wbr</td>\n",
       "      <td>positive</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Building the perfect tracklist to life leave the streets ablaze</td>\n",
       "      <td>positive</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http://t.co/3Tj8ZjiN21 http://t.co/YDUiXEfIpE http://t.co/LxTjc87KLS #nsfw</td>\n",
       "      <td>negative</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>First night with retainers in. It's quite weird. Better get used to it; I have to wear them every single night for the next year at least.</td>\n",
       "      <td>negative</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Deputies: Man shot before Brighton home set ablaze http://t.co/gWNRhMSO8k</td>\n",
       "      <td>negative</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            text sentiment keyword sentiment_kw\n",
       "31                                                                                       @bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C  positive  ablaze     positive\n",
       "32                                                                           We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw  positive  ablaze     positive\n",
       "33                                                            #AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi  positive  ablaze     positive\n",
       "34                                                                                                            Crying out for more! Set me ablaze  positive  ablaze     positive\n",
       "35                                                                  On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N  positive  ablaze     positive\n",
       "36                    @PhDSquares #mufc they've built so much hype around new acquisitions but I doubt they will set the EPL ablaze this season.  positive  ablaze     positive\n",
       "37                                                                                       INEC Office in Abia Set Ablaze - http://t.co/3ImaomknnA  positive  ablaze     positive\n",
       "38  Barbados #Bridgetown JAMAICA  Two cars set ablaze: SANTA CRUZ  Head of the St Elizabeth Police Superintende...  http://t.co/wDUEaj8Q4J  positive  ablaze     positive\n",
       "39                                                                                                                        Ablaze for you Lord :D  positive  ablaze     positive\n",
       "40                            Check these out: http://t.co/rOI2NSmEJJ http://t.co/3Tj8ZjiN21 http://t.co/YDUiXEfIpE http://t.co/LxTjc87KLS #nsfw  negative  ablaze     negative\n",
       "41                                                                                on the outside you're ablaze and alive\\nbut you're dead inside  positive  ablaze     positive\n",
       "42                      Had an awesome time visiting the CFC head office the ancop site and ablaze. Thanks to Tita Vida for taking care of us ??  negative  ablaze     negative\n",
       "43                                                                                                  SOOOO PUMPED FOR ABLAZE ???? @southridgelife  negative  ablaze     negative\n",
       "44                                                  I wanted to set Chicago ablaze with my preaching... But not my hotel! http://t.co/o9qknbfOFX  negative  ablaze     negative\n",
       "45                                              I gained 3 followers in the last week. You? Know your stats and grow with http://t.co/TIyUliF5c6  negative  ablaze     negative\n",
       "46                                             How the West was burned: Thousands of wildfires ablaze in California alone http://t.co/vl5TBR3wbr  positive  ablaze     positive\n",
       "47                                                                               Building the perfect tracklist to life leave the streets ablaze  positive  ablaze     positive\n",
       "48                            Check these out: http://t.co/rOI2NSmEJJ http://t.co/3Tj8ZjiN21 http://t.co/YDUiXEfIpE http://t.co/LxTjc87KLS #nsfw  negative  ablaze     negative\n",
       "49    First night with retainers in. It's quite weird. Better get used to it; I have to wear them every single night for the next year at least.  negative  ablaze     negative\n",
       "50                                                                     Deputies: Man shot before Brighton home set ablaze http://t.co/gWNRhMSO8k  negative  ablaze     negative"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.keyword!=\"missing\"][[\"text\", \"sentiment\", \"keyword\", \"sentiment_kw\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">negative</th>\n",
       "      <th>0</th>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">positive</th>\n",
       "      <th>0</th>\n",
       "      <td>2383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0\n",
       "sentiment target      \n",
       "negative  0       1959\n",
       "          1       1328\n",
       "positive  0       2383\n",
       "          1       1943"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.groupby([\"sentiment\", \"target\"]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 9) (2284, 9)\n"
     ]
    }
   ],
   "source": [
    "Xcol=\"text\"\n",
    "ycol=\"target\"\n",
    "\n",
    "df[Xcol] = df[Xcol].apply(lambda x: ((x.encode(\"unicode_escape\").decode(\"utf-8\"))))\n",
    "df[Xcol] = df[Xcol].apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))# str.translate(str.maketrans('','',string.punctuation))\n",
    "df[Xcol] = df[Xcol].str.lower().str.replace('\\n',' ')\n",
    "df[ycol] =df[ycol].astype(str)\n",
    "\n",
    "train_df = df.sample(frac=0.70, random_state=0)\n",
    "test_df  = df.drop(train_df.index)\n",
    "\n",
    "\n",
    "train_df[\"fasttext_trn\"] = '__label__'+train_df[ycol]+' '+train_df[Xcol]\n",
    "test_df[\"fasttext_test\"] = test_df[Xcol].copy()\n",
    "\n",
    "print(train_df.shape, test_df.shape)\n",
    "\n",
    "f = open(f'{PATH}data.train','w')\n",
    "f.write('\\n'.join(train_df.fasttext_trn.tolist()))\n",
    "f.close()\n",
    "\n",
    "f = open(f'{PATH}data.valid','w')\n",
    "f.write('\\n'.join(test_df.fasttext_test.tolist()))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = fasttext.train_supervised(f'{PATH}data.train',lr=0.05,epoch=50,word_ngrams=3,bucket=200000,dim=100,loss='softmax')\n",
    "classifier.save_model(f'{PATH}myfasttextmodel.bin')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'k' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-305-4566ace13978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasttext_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__label__0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__label__1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.88878089\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.11123914\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#classifier.predict(txt, k=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlbls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'k' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "for txt in test_df.fasttext_test.sample(1):  \n",
    "    print(classifier.predict(txt), k=2)\n",
    "pred=(('__label__0', '__label__1'), np.array([0.88878089, 0.11123914]))#classifier.predict(txt, k=2)\n",
    "print(pred)\n",
    "lbls=[pred[0][0][9:],pred[0][1][9:]]\n",
    "prds=[pred[1][0],pred[1][1]]\n",
    "print(lbls)\n",
    "print(prds)\n",
    "dict(zip(lbls, prds))[\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__0', '__label__1'), array([0.88878089, 0.11123914]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=(('__label__0', '__label__1'), np.array([0.88878089, 0.11123914]))#classifier.predict(txt, k=2)\n",
    "print(pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "accuracy:  0.79\n",
      "************************************************************\n",
      "classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.89      0.83      1292\n",
      "           1       0.82      0.66      0.73       992\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      2284\n",
      "   macro avg       0.80      0.77      0.78      2284\n",
      "weighted avg       0.79      0.79      0.79      2284\n",
      "\n",
      "************************************************************\n",
      "confusion matrix: \n",
      " col_0     0    1\n",
      "row_0           \n",
      "0      1148  144\n",
      "1       336  656\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "y_pred=[]\n",
    "y_probs=[]\n",
    "for txt in test_df.fasttext_test:  \n",
    "    pred=classifier.predict(txt, k=2)\n",
    "    max_lbl=pred[0][0][9:]\n",
    "    \n",
    "    lbls=[pred[0][0][9:],pred[0][1][9:]]\n",
    "    prds=[pred[1][0],pred[1][1]]\n",
    "    prob=dict(zip(lbls, prds))[\"1\"]\n",
    "    \n",
    "    y_pred.append(max_lbl)\n",
    "    y_probs.append(prob)\n",
    "\n",
    "    \n",
    "y_pred = np.array(y_pred, dtype='O')\n",
    "y_true = np.array(y_true, dtype='O')\n",
    "len(y_pred), len(y_true)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"*\"*60)\n",
    "print(\"accuracy: \", round(accuracy_score(y_true,y_pred), 2))\n",
    "print(\"*\"*60)\n",
    "print(\"classification report: \\n\", classification_report(y_true, y_pred))\n",
    "print(\"*\"*60)\n",
    "print(\"confusion matrix: \\n\", pd.crosstab(pd.Series(y_true), pd.Series(y_pred)))\n",
    "print(\"*\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current f1:  0.73\n"
     ]
    }
   ],
   "source": [
    "current_f1=metrics.f1_score(y_true=y_true.astype(int), y_pred=y_pred.astype(int))\n",
    "print(\"current f1: \", round(current_f1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:02<00:00, 469.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold:  0.297\n",
      "f1 at best threshold:  0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.001 for i in range(1000)]):\n",
    "        y_pred_prob=np.where(y_proba > threshold, 1, 0)\n",
    "        score = metrics.f1_score(y_true=y_true, y_pred=y_pred_prob)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result\n",
    "\n",
    "search_result = threshold_search(np.array(y_true.astype(int)),np.array(y_probs))\n",
    "\n",
    "print(\"best threshold: \", search_result[\"threshold\"])\n",
    "print(\"f1 at best threshold: \", round(search_result[\"f1\"], 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important  Words Used For Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "eli5.show_weights(model, vec=test_tfidf[0], top=50, feature_filter=lambda x: x != '<BIAS>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Explainability with LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LIME ( Local Interpretable Model-agnostic Explanations )is a novel explanation technique that explains the prediction of any classifier in an interpretable by learning a interpretable model locally around the prediction.\n",
    "\n",
    "\n",
    "- For NLP, LIME explains what keywords were focused to during the prediction.\n",
    "\n",
    "\n",
    "- LIME works for only binary class classifcation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import List, Any\n",
    "\n",
    "import numpy as np\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import sklearn.pipeline\n",
    "import scipy.stats\n",
    "import spacy\n",
    "\n",
    "\n",
    "METHODS = {\n",
    "    'fasttext': {\n",
    "        'class': \"FastTextExplainer\",\n",
    "        'file': \"mymodel.bin\"\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def tokenizer(text: str) -> str:\n",
    "    \"Tokenize input string using a spaCy pipeline\"\n",
    "    nlp = spacy.blank('en')\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))  # Very basic NLP pipeline in spaCy\n",
    "    doc = nlp(text)\n",
    "    tokenized_text = ' '.join(token.text for token in doc)\n",
    "    return tokenized_text\n",
    "\n",
    "\n",
    "def explainer_class(method: str, filename: str) -> Any:\n",
    "    \"Instantiate class using its string name\"\n",
    "    classname = METHODS[method]['class']\n",
    "    class_ = globals()[classname]\n",
    "    return class_(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FastTextExplainer:\n",
    "    \"\"\"Class to explain classification results of FastText.\n",
    "       Assumes that we already have a trained FastText model with which to make predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_to_model: str) -> None:\n",
    "        \"Input fastText trained sentiment model\"\n",
    "        import fasttext\n",
    "        self.classifier = fasttext.load_model(path_to_model)\n",
    "\n",
    "    def predict(self, texts: List[str]) -> np.array([float, ...]):\n",
    "        \"Generate an array of predicted scores using the FastText\"\n",
    "        labels, probs = self.classifier.predict(texts, 5)\n",
    "\n",
    "        result = []\n",
    "        for label, prob, text in zip(labels, probs, texts):\n",
    "            order = np.argsort(np.array(label))\n",
    "            result.append(prob[order])\n",
    "        return np.array(result)\n",
    "\n",
    "\n",
    "def explainer(method: str,\n",
    "              path_to_file: str,\n",
    "              text: str,\n",
    "              num_samples: int) -> LimeTextExplainer:\n",
    "\n",
    "    model = explainer_class(method, path_to_file)\n",
    "    predictor = model.predict\n",
    "\n",
    "    # Create a LimeTextExplainer\n",
    "    explainer = LimeTextExplainer(\n",
    "        # Specify split option\n",
    "        split_expression=lambda x: x.split(),\n",
    "        bow=False,\n",
    "#         class_names=[1, 2, 3, 4, 5]\n",
    "        class_names=train[\"target\"].unique().tolist()\n",
    "    )\n",
    "\n",
    "    exp = explainer.explain_instance(\n",
    "        text,\n",
    "        classifier_fn=predictor,\n",
    "        top_labels=1,\n",
    "        num_features=5,\n",
    "        num_samples=num_samples,\n",
    "    )\n",
    "    return exp\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'this is ridiculous....')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=train.iloc[20][[\"text\", \"target\"]]\n",
    "text=r[\"text\"]\n",
    "label=r[\"target\"]\n",
    "label,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer(\"fasttext\", f\"{PATH}myfasttextmodel.bin\", text, 1000)\n",
    "# exp.save_to_file(\"explained.html\") # save output as html file\n",
    "exp.show_in_notebook(text=text,predict_proba=True, show_predicted_value=True)#, labels=(\"0\",))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data/imgs/lime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (faisalenv)",
   "language": "python",
   "name": "faisalenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
